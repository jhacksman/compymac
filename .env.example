# LLM Configuration
# -----------------
# The agent works with any OpenAI-compatible API:
# - vLLM: http://localhost:8000/v1
# - Ollama: http://localhost:11434/v1
# - Venice.ai: https://api.venice.ai/api/v1
# - OpenAI: https://api.openai.com/v1

# Default: vLLM (local, high-throughput serving)
# LLM_BASE_URL=http://localhost:8000/v1
# LLM_API_KEY=
# LLM_MODEL=<your-model-name>

# Ollama (local development, simpler setup)
# LLM_BASE_URL=http://localhost:11434/v1
# LLM_API_KEY=ollama
# LLM_MODEL=llama3.2

# Venice.ai Configuration (recommended for production)
# Available models with function calling:
# - qwen3-coder-480b-a35b-instruct: 262k context, best for code/agents ($0.75/$3.00 per 1M tokens)
# - llama-3.3-70b: 131k context, stable default ($0.70/$2.80 per 1M tokens)
# - qwen3-235b: 131k context, most powerful reasoning ($0.90/$4.50 per 1M tokens)
# - mistral-31-24b: 131k context, vision + function calling ($0.50/$2.00 per 1M tokens)
LLM_BASE_URL=https://api.venice.ai/api/v1
LLM_API_KEY=your-venice-api-key
LLM_MODEL=qwen3-coder-480b-a35b-instruct
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=4096

# Context Configuration
# ---------------------
# These settings control the fundamental context window constraint.
# The token budget is a HARD LIMIT - when exceeded, oldest messages
# are dropped (naive truncation, not summarization).

CONTEXT_TOKEN_BUDGET=128000
CONTEXT_CHARS_PER_TOKEN=4.0
CONTEXT_RESERVED_FOR_RESPONSE=4096

# Agent Loop Configuration
# ------------------------
# max_steps is a safety limit to prevent runaway execution.
# This is an operational constraint that exists in real agent systems.

AGENT_MAX_STEPS=20
