# LLM Configuration
# -----------------
# The agent works with any OpenAI-compatible API:
# - vLLM: http://localhost:8000/v1
# - Ollama: http://localhost:11434/v1
# - Venice.ai: https://api.venice.ai/api/v1
# - OpenAI: https://api.openai.com/v1

# Default: Ollama (local)
# LLM_BASE_URL=http://localhost:11434/v1
# LLM_API_KEY=ollama
# LLM_MODEL=llama3.2

# Venice.ai Configuration (recommended for production)
# 
# RECOMMENDED MODELS (235B series):
# - qwen3-235b-a22b-instruct-2507: Best for regular/light work (coding, agents, general tasks)
# - qwen3-235b-a22b-thinking-2507: Best for heavy reasoning (complex analysis, planning)
#
# Other available models:
# - mistral-31-24b: 131k context, vision + function calling ($0.50/$2.00 per 1M tokens)
# - qwen3-coder-480b-a35b-instruct: 262k context, code-focused ($0.75/$3.00 per 1M tokens)
LLM_BASE_URL=https://api.venice.ai/api/v1
LLM_API_KEY=your-venice-api-key
LLM_MODEL=qwen3-235b-a22b-instruct-2507
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=4096

# Context Configuration
# ---------------------
# These settings control the fundamental context window constraint.
# The token budget is a HARD LIMIT - when exceeded, oldest messages
# are dropped (naive truncation, not summarization).

CONTEXT_TOKEN_BUDGET=128000
CONTEXT_CHARS_PER_TOKEN=4.0
CONTEXT_RESERVED_FOR_RESPONSE=4096

# Agent Loop Configuration
# ------------------------
# max_steps is a safety limit to prevent runaway execution.
# This is an operational constraint that exists in real agent systems.

AGENT_MAX_STEPS=20
