# LLM Configuration
# -----------------
# The agent works with any OpenAI-compatible API:
# - vLLM: http://localhost:8000/v1
# - Ollama: http://localhost:11434/v1
# - Venice.ai: https://api.venice.ai/api/v1
# - OpenAI: https://api.openai.com/v1

LLM_BASE_URL=http://localhost:11434/v1
LLM_API_KEY=ollama
LLM_MODEL=llama3.2
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=4096

# Context Configuration
# ---------------------
# These settings control the fundamental context window constraint.
# The token budget is a HARD LIMIT - when exceeded, oldest messages
# are dropped (naive truncation, not summarization).

CONTEXT_TOKEN_BUDGET=128000
CONTEXT_CHARS_PER_TOKEN=4.0
CONTEXT_RESERVED_FOR_RESPONSE=4096

# Agent Loop Configuration
# ------------------------
# max_steps is a safety limit to prevent runaway execution.
# This is an operational constraint that exists in real agent systems.

AGENT_MAX_STEPS=20
